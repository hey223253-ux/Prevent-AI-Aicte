"""
PreventAI – Model Evaluation Module
======================================
Cross-validation, metrics computation, and model comparison.
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, make_scorer
)
import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from models.model_utils import compute_metrics, print_metrics


def evaluate_model(model, X_test, y_test, model_name="Model"):
    """Evaluate a single model on test data."""
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    metrics = compute_metrics(y_test, y_pred, y_prob)
    print_metrics(metrics, model_name)
    return metrics


def cross_validate_model(model, X, y, cv=5, model_name="Model"):
    """Perform stratified k-fold cross-validation with multiple metrics."""
    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)

    scoring = {
        'accuracy': 'accuracy',
        'precision': make_scorer(precision_score, zero_division=0),
        'recall': make_scorer(recall_score, zero_division=0),
        'f1': make_scorer(f1_score, zero_division=0),
        'roc_auc': 'roc_auc'
    }

    cv_results = cross_validate(
        model, X, y,
        cv=skf,
        scoring=scoring,
        n_jobs=-1,
        return_train_score=False
    )

    print(f"\n{'='*50}")
    print(f"  {model_name} – {cv}-Fold Cross-Validation")
    print(f"{'='*50}")
    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:
        key = f'test_{metric}'
        if key in cv_results:
            scores = cv_results[key]
            print(f"  {metric:>12}: {scores.mean():.4f} ± {scores.std():.4f}")

    return cv_results


def compare_models(all_results, output_path=None):
    """
    Compare all models across all targets and generate report.

    Parameters:
        all_results: dict of {model_name: {target: metrics_dict}}
        output_path: path to save report markdown
    """
    targets = ['diabetes_risk', 'cvd_risk', 'hypertension_risk']
    target_labels = {
        'diabetes_risk': 'Type 2 Diabetes',
        'cvd_risk': 'Cardiovascular Disease',
        'hypertension_risk': 'Hypertension'
    }

    report_lines = []
    report_lines.append("# PreventAI – Model Evaluation Report")
    report_lines.append(f"\n_Generated by PreventAI evaluation pipeline_\n")

    # Medical Disclaimer
    report_lines.append("---")
    report_lines.append("\n> ⚠️ **MEDICAL DISCLAIMER**: This system is for educational "
                        "and research purposes only. It is NOT a substitute for "
                        "professional medical advice, diagnosis, or treatment. "
                        "Always consult a qualified healthcare provider.\n")
    report_lines.append("---\n")

    # Summary table for each target
    for target in targets:
        if not any(target in r for r in all_results.values()):
            continue

        report_lines.append(f"\n## {target_labels.get(target, target)}\n")
        report_lines.append("| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |")
        report_lines.append("|-------|----------|-----------|--------|----------|---------|")

        best_auc = 0
        best_model = ""

        for model_name in sorted(all_results.keys()):
            if target in all_results[model_name]:
                m = all_results[model_name][target]
                auc = m.get('roc_auc', 0)
                if auc > best_auc:
                    best_auc = auc
                    best_model = model_name

                report_lines.append(
                    f"| {model_name} | {m['accuracy']:.4f} | "
                    f"{m['precision']:.4f} | {m['recall']:.4f} | "
                    f"{m['f1_score']:.4f} | {auc:.4f} |"
                )

        if best_model:
            report_lines.append(f"\n**Best Model**: {best_model} (ROC-AUC: {best_auc:.4f})\n")

    # Save report
    report = "\n".join(report_lines)

    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\n  ✓ Evaluation report saved to {output_path}")

    return report
